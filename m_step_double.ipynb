{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9dbe5b-c63e-4c20-b71d-ae06edb34869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "# import seaborn as sns\n",
    "from numpy import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import original_data\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# config\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 400\n",
    "N_FRAME = 12\n",
    "TIME_LENGTH = 12\n",
    "N_SAMPLE = 10000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PATH_1 = './data/timedt.dataHe1.300'\n",
    "PATH_2 = './data/timedt.dataHe2.300'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a8b4db2-6359-4b80-8581-519e31439785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_data(\n",
    "   path:str\n",
    "):\n",
    "    \"\"\"\n",
    "    input temperature : 温度\n",
    "    \n",
    "    return t, msd, csp, xyz, r_, v_xyz, v_, angle, g\n",
    "\n",
    "    t       时间序列(单位:ps)\n",
    "    msd     单He的msd(均方位移)\n",
    "    csp     CSP(中心对称参数)\n",
    "    xyz     单He的xyz坐标\n",
    "    r_      单He离原点距离\n",
    "    v_xyz   单He的沿xyz坐标的速度分量\n",
    "    v_      单He的速度大小\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as fin:\n",
    "        t = []      # 时间序列(单位:ps)\n",
    "        msd = []    # 单He的msd(均方位移)\n",
    "        csp = []    # CSP(中心对称参数)\n",
    "        xyz = []    # 单He的xyz坐标\n",
    "        r_ = []     # 单He离原点距离\n",
    "        v_xyz = []  # 单He的沿xyz坐标的速度分量\n",
    "        v_ = []     # 单He的速度大小\n",
    "        for i, line in enumerate(fin.readlines()[1:]):\n",
    "            data = list(map(float, line.strip().split(' ')))\n",
    "            t.append(data[0])\n",
    "            msd.append(data[1])\n",
    "            csp.append(data[2:8])\n",
    "            xyz.append(data[8:11])\n",
    "            r_.append(data[11])\n",
    "            v_xyz.append(data[12:15])\n",
    "            v_.append(data[15])\n",
    "\n",
    "    # with open(G_PATH.format(Temperature=temperature), 'r', encoding='utf-8') as fin:\n",
    "    #     g = []      # g参数\n",
    "    #     for i, line in enumerate(fin.readlines()[1:]):\n",
    "    #         data = list(map(float, line.strip().split(' ')))\n",
    "    #         g.append(data[1:7])\n",
    "    indices_to_remove = np.arange(1001, len(t) - 1, 1001)\n",
    "    t = np.array(t)\n",
    "    t = np.delete(t, indices_to_remove)\n",
    "    t = t.reshape(-1, 1)\n",
    "    msd = np.array(msd)\n",
    "    msd = np.delete(msd, indices_to_remove)\n",
    "    msd = msd.reshape(-1, 1)\n",
    "    csp = np.array(csp)\n",
    "    csp = np.delete(csp, indices_to_remove, axis=0)\n",
    "    xyz = np.array(xyz)\n",
    "    xyz = np.delete(xyz, indices_to_remove, axis=0)\n",
    "    r_ = np.sqrt(np.array(r_))\n",
    "    r_ = np.delete(r_, indices_to_remove)\n",
    "    r_ = r_.reshape(-1, 1)\n",
    "    v_xyz = np.array(v_xyz)\n",
    "    v_xyz = np.delete(v_xyz, indices_to_remove, axis=0)\n",
    "    v_ = np.sqrt(np.array(v_))\n",
    "    v_ = np.delete(v_, indices_to_remove)\n",
    "    v_ = v_.reshape(-1, 1)\n",
    "    angle = np.arccos(v_xyz / v_.reshape(len(t), 1))\n",
    "    # g = np.array(g)\n",
    "\n",
    "    return t, msd, csp, xyz, r_, v_xyz, v_, angle# , g\n",
    "# TRAIN\n",
    "def train(\n",
    "    net:nn.Module, \n",
    "    Train_generator, \n",
    "    loss_func,\n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    device\n",
    "):\n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "    sum_loss = []\n",
    "    \n",
    "    Train_generator =  DataLoader(Train_generator.dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "    for x, y in Train_generator:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        scores = net(x)\n",
    "        loss = loss_func(scores, y)\n",
    "        sum_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return np.array(sum_loss).mean()\n",
    "    \n",
    "def evaluate(\n",
    "    net:nn.Module, \n",
    "    Test_generator, \n",
    "    loss_func,\n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    device\n",
    "):\n",
    "    sum_loss = []\n",
    "    net.eval()\n",
    "    Test_generator = DataLoader(Test_generator.dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "    for x, y in Test_generator:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        scores = net(x)\n",
    "        loss = loss_func(scores, y)\n",
    "        sum_loss.append(loss.item())\n",
    "    \n",
    "    return np.array(sum_loss).mean()\n",
    "\n",
    "\n",
    "def train_multi_epochs(\n",
    "    path, \n",
    "    net:nn.Module, \n",
    "    Train_generator, \n",
    "    Test_generator,     \n",
    "    loss_func,\n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    epochs, \n",
    "    device, \n",
    "    information:str, \n",
    "    show_train_process=None\n",
    "):\n",
    "    best_epoch = 0\n",
    "    best_test_loss = 9.9e9\n",
    "    best_net = net.state_dict()\n",
    "    sum_train_loss, sum_test_loss = [], []\n",
    "    date0 = time.strftime('%Y-%m-%d %a %H-%M-%S', time.localtime(time.time()))\n",
    "\n",
    "    with open(path.format(date=date0, information=information), 'w') as log_fin:\n",
    "        log_fin.write(information + '\\n')\n",
    "        log_fin.write('epoch' + ' ' + 'train_loss' + ' ' + 'test_loss' + ' ' + 'time' + ' ' + 'best_epoch' + '\\n')\n",
    "        t1 = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t0 = time.time()\n",
    "            train_loss = train(net, Train_generator, loss_func, optimizer, scheduler, device).item()\n",
    "            test_loss = evaluate(net, Test_generator, loss_func, optimizer, scheduler, device).item()\n",
    "\n",
    "            sum_train_loss.append(train_loss)\n",
    "            sum_test_loss.append(test_loss)\n",
    "\n",
    "            if epoch == 0 or test_loss < best_test_loss:\n",
    "                best_epoch = epoch\n",
    "                best_test_loss = test_loss\n",
    "                best_net = net.state_dict()\n",
    "\n",
    "            log_fin.write(str(epoch) + ' ' + str(train_loss) + ' ' + str(test_loss) + ' ' + str(time.time()-t0) + ' ' + str(best_epoch) + '\\n')\n",
    "            if show_train_process != None and epoch % show_train_process == 0:\n",
    "                print('epoch={:>4}, train_loss= {:.4f}, test_loss= {:.4f}, time= {:.2f}sec, best_epoch= {:>4}'.format(epoch, train_loss, test_loss, time.time()-t1, best_epoch))\n",
    "                t1 = time.time()\n",
    "        \n",
    "        log_fin.write('\\n')\n",
    "        log_fin.write('best_epoch=' + str(best_epoch) + '\\n')\n",
    "        log_fin.write('best_test_loss=' + str(best_test_loss) + '\\n')\n",
    "    return best_test_loss, best_epoch, best_net, sum_train_loss, sum_test_loss\n",
    "\n",
    "\n",
    "def singal_train_CNN(i, lr=LEARNING_RATE, ga=0.5, dropout=0.5):\n",
    "    t1 = time.time()\n",
    "    # train_iter, test_iter = get_train_iter(TEMPERATURE, BATCH_SIZE)\n",
    "    train_iter = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_iter = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # test_iter = DataLoader([final_data, final_labels], batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "    # criteon = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "    criteon = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    # net = CNN(1, TIME_LENGTH, INPUT_SIZE, 1, 32, [3, 5, 7, 9], dropout)\n",
    "    net = CNN(1,N_FRAME, 37,4, 8, [9,7,5,3], dropout)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=0)\n",
    "    scheduler = MultiStepLR(optimizer, [int(EPOCHS*0.2), int(EPOCHS*0.4)], ga, last_epoch=-1)\n",
    "\n",
    "    best_test_loss, best_epoch, _, multi_train_loss, multi_test_loss = train_multi_epochs(f'./output/{i}', net, train_iter, test_iter, criteon, optimizer, scheduler, EPOCHS, DEVICE, 'Temperature={}'.format(300), show_train_process=10)\n",
    "\n",
    "    print('best_test_loss= {:.4f}, best_epoch= {:>4}, time= {:.2f}sec'.format(best_test_loss, best_epoch, time.time()-t1))\n",
    "\n",
    "    # plt.plot(multi_train_loss)\n",
    "    # plt.plot(multi_test_loss)\n",
    "    # plt.savefig(f'./output/{i}.png')\n",
    "    \n",
    "    return multi_train_loss, multi_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5870f37-f375-4829-a5f6-7fd12311a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iter(path1, path2,  Batch_size):\n",
    "    '''\n",
    "    这个index有没有用另外说\n",
    "    '''\n",
    "    data1 = get_original_data(path1)\n",
    "    data2 = get_original_data(path2)\n",
    "    y = np.hstack([data1[1],data2[1]])\n",
    "    # index = np.where(label[TIME_LENGTH//2 : -TIME_LENGTH//2])[0] + TIME_LENGTH//2\n",
    "    data_num = len(data1[0])\n",
    "    data1 = np.hstack(data1)\n",
    "    data2 = np.hstack(data2)\n",
    "    data = np.hstack((data1[:,1:], data2[:,1:]))\n",
    "    INPUT_SIZE = data.shape[-1]\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    data_X = torch.from_numpy(data)\n",
    "    # print(data_X.shape)\n",
    "    data_Y = torch.from_numpy(y)\n",
    "    data_Y = data_Y\n",
    "    \n",
    "    X = torch.zeros(N_SAMPLE, TIME_LENGTH//2, INPUT_SIZE)\n",
    "    Y = torch.zeros(N_SAMPLE, TIME_LENGTH//2, 2)\n",
    "    \n",
    "    import random\n",
    "    \n",
    "    start_index = TIME_LENGTH // 2\n",
    "    end_index = data_num - TIME_LENGTH // 2\n",
    "    sample_indices = random.sample(range(start_index, end_index), N_SAMPLE)\n",
    "    data_num = N_SAMPLE\n",
    "    \n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        X[i, :, :] = data_X[idx-TIME_LENGTH//2 : idx]\n",
    "        Y[i, :, :]    = data_Y[idx:idx+TIME_LENGTH//2]\n",
    "\n",
    "    X = X.float().unsqueeze(1) # [Batch_size, C, H, W]\n",
    "    shuffled_index = np.random.permutation(range(data_num))\n",
    "    X = X[shuffled_index]\n",
    "    Y = Y[shuffled_index]\n",
    "    Y = Y.view(Y.size(0), -1)\n",
    "    Train_X, Test_X = X[:int(data_num*0.8)], X[int(data_num*0.8):]\n",
    "    Train_Y, Test_Y = Y[:int(data_num*0.8)], Y[int(data_num*0.8):]\n",
    "\n",
    "\n",
    "    Train_generator = DataLoader(\n",
    "        torch.utils.data.TensorDataset(Train_X, Train_Y), \n",
    "        Batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    Test_generator = DataLoader(\n",
    "        torch.utils.data.TensorDataset(Test_X, Test_Y), \n",
    "        Batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    return Train_generator, Test_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bd5285b6-02ad-4c0d-bb8f-387d92a02b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        Channel_in, \n",
    "        Height_in, \n",
    "        Width_in, \n",
    "        Output_size, \n",
    "        Filter_num, \n",
    "        Kernel_list, \n",
    "        dropout = 0.5, \n",
    "    ):\n",
    "        super(CNN, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    Channel_in, \n",
    "                    Filter_num, \n",
    "                    kernel_size=(kernel, Width_in), \n",
    "                    padding=((kernel - 1) // 2, 0), \n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(\n",
    "                    kernel_size=((Height_in+3)//4, 1), \n",
    "                    stride=(Height_in+3)//4, \n",
    "                    padding=((Height_in-Height_in//4*4+1)//2, 0), \n",
    "                ), \n",
    "            )\n",
    "            for kernel in Kernel_list\n",
    "        ])\n",
    "        # print(Kernel_list)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout), \n",
    "            nn.Linear(Filter_num * len(Kernel_list) * 4, 64),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, 16), \n",
    "            nn.Linear(16, Output_size)\n",
    "        )\n",
    "        # one -hot 时就这样就行 output_size=4\n",
    "        # self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        out = [conv(x) for conv in self.convs]\n",
    "        out = torch.cat(out, dim=1)\n",
    "        out = out.view(x.size(0), -1)\n",
    "        # print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        # output = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0cef23b3-473f-490d-baf0-218775e76b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    net:nn.Module, \n",
    "    Train_generator, \n",
    "    loss_func,\n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    device\n",
    "):\n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "    sum_loss = []\n",
    "    \n",
    "    Train_generator =  DataLoader(Train_generator.dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "    for x, y in Train_generator:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        scores = net(x)\n",
    "        loss = loss_func(scores, y)\n",
    "        sum_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return np.array(sum_loss).mean()\n",
    "    \n",
    "def evaluate(\n",
    "    net:nn.Module, \n",
    "    Test_generator, \n",
    "    loss_func,\n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    device\n",
    "):\n",
    "    sum_loss = []\n",
    "    net.eval()\n",
    "    Test_generator = DataLoader(Test_generator.dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "    for x, y in Test_generator:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        scores = net(x)\n",
    "        loss = loss_func(scores, y)\n",
    "        sum_loss.append(loss.item())\n",
    "    \n",
    "    return np.array(sum_loss).mean()\n",
    "\n",
    "\n",
    "def train_multi_epochs(\n",
    "    path, \n",
    "    net:nn.Module, \n",
    "    Train_generator, \n",
    "    Test_generator,     \n",
    "    loss_func,\n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    epochs, \n",
    "    device, \n",
    "    information:str, \n",
    "    show_train_process=None\n",
    "):\n",
    "    best_epoch = 0\n",
    "    best_test_loss = 9.9e9\n",
    "    best_net = net.state_dict()\n",
    "    sum_train_loss, sum_test_loss = [], []\n",
    "    date0 = time.strftime('%Y-%m-%d %a %H-%M-%S', time.localtime(time.time()))\n",
    "\n",
    "    with open(path.format(date=date0, information=information), 'w') as log_fin:\n",
    "        log_fin.write(information + '\\n')\n",
    "        log_fin.write('epoch' + ' ' + 'train_loss' + ' ' + 'test_loss' + ' ' + 'time' + ' ' + 'best_epoch' + '\\n')\n",
    "        t1 = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t0 = time.time()\n",
    "            train_loss = train(net, Train_generator, loss_func, optimizer, scheduler, device).item()\n",
    "            test_loss = evaluate(net, Test_generator, loss_func, optimizer, scheduler, device).item()\n",
    "\n",
    "            sum_train_loss.append(train_loss)\n",
    "            sum_test_loss.append(test_loss)\n",
    "\n",
    "            if epoch == 0 or test_loss < best_test_loss:\n",
    "                best_epoch = epoch\n",
    "                best_test_loss = test_loss\n",
    "                best_net = net.state_dict()\n",
    "\n",
    "            log_fin.write(str(epoch) + ' ' + str(train_loss) + ' ' + str(test_loss) + ' ' + str(time.time()-t0) + ' ' + str(best_epoch) + '\\n')\n",
    "            if show_train_process != None and epoch % show_train_process == 0:\n",
    "                print('epoch={:>4}, train_loss= {:.4f}, test_loss= {:.4f}, time= {:.2f}sec, best_epoch= {:>4}'.format(epoch, train_loss, test_loss, time.time()-t1, best_epoch))\n",
    "                t1 = time.time()\n",
    "        \n",
    "        log_fin.write('\\n')\n",
    "        log_fin.write('best_epoch=' + str(best_epoch) + '\\n')\n",
    "        log_fin.write('best_test_loss=' + str(best_test_loss) + '\\n')\n",
    "    return best_test_loss, best_epoch, best_net, sum_train_loss, sum_test_loss\n",
    "\n",
    "\n",
    "def singal_train_CNN(i, lr=LEARNING_RATE, ga=0.5, dropout=0.5):\n",
    "    t1 = time.time()\n",
    "    train_iter, test_iter = get_iter(PATH_1,PATH_2, BATCH_SIZE)\n",
    "    # test_iter = DataLoader([final_data, final_labels], batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "    # criteon = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "    criteon = nn.MSELoss().to(DEVICE)\n",
    "    # net = CNN(1, TIME_LENGTH, INPUT_SIZE, 1, 32, [3, 5, 7, 9], dropout)\n",
    "    net = CNN(1,TIME_LENGTH//2 , 18 * 2,TIME_LENGTH//2 * 2, 8, [9,7,5,3], dropout)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=0)\n",
    "    scheduler = MultiStepLR(optimizer, [int(EPOCHS*0.2), int(EPOCHS*0.4)], ga, last_epoch=-1)\n",
    "\n",
    "    best_test_loss, best_epoch, _, multi_train_loss, multi_test_loss = train_multi_epochs(f'./output/{i}', net, train_iter, test_iter, criteon, optimizer, scheduler, EPOCHS, DEVICE, 'Temperature={}'.format(300), show_train_process=10)\n",
    "\n",
    "    print('best_test_loss= {:.4f}, best_epoch= {:>4}, time= {:.2f}sec'.format(best_test_loss, best_epoch, time.time()-t1))\n",
    "\n",
    "    plt.plot(multi_train_loss)\n",
    "    plt.plot(multi_test_loss)\n",
    "    plt.savefig(f'./output/{i}.png')\n",
    "    \n",
    "    return multi_train_loss, multi_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2aa2e134-76d6-4e12-a796-f29c6c5fb62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=   0, train_loss= 1286058.2393, test_loss= 1243289.2810, time= 1.62sec, best_epoch=    0\n",
      "epoch=  10, train_loss= 223879.6325, test_loss= 195455.6891, time= 16.21sec, best_epoch=   10\n",
      "epoch=  20, train_loss= 137875.0418, test_loss= 116191.0842, time= 16.20sec, best_epoch=   20\n",
      "epoch=  30, train_loss= 120958.0969, test_loss= 98684.6490, time= 16.32sec, best_epoch=   30\n",
      "epoch=  40, train_loss= 112841.9315, test_loss= 91143.8570, time= 16.27sec, best_epoch=   40\n",
      "epoch=  50, train_loss= 109004.3152, test_loss= 87955.6585, time= 16.22sec, best_epoch=   50\n",
      "epoch=  60, train_loss= 106929.9873, test_loss= 86557.6183, time= 16.32sec, best_epoch=   60\n",
      "epoch=  70, train_loss= 106314.9945, test_loss= 85782.6281, time= 16.28sec, best_epoch=   67\n",
      "epoch=  80, train_loss= 105139.8698, test_loss= 85328.4378, time= 16.23sec, best_epoch=   80\n",
      "epoch=  90, train_loss= 104198.5601, test_loss= 84846.5760, time= 16.23sec, best_epoch=   86\n",
      "epoch= 100, train_loss= 103045.1592, test_loss= 84494.3420, time= 16.23sec, best_epoch=   97\n",
      "epoch= 110, train_loss= 103288.3176, test_loss= 84228.0787, time= 16.19sec, best_epoch=  107\n",
      "epoch= 120, train_loss= 102846.0550, test_loss= 83924.7209, time= 16.31sec, best_epoch=  119\n",
      "epoch= 130, train_loss= 101446.6485, test_loss= 83807.9695, time= 16.21sec, best_epoch=  129\n",
      "epoch= 140, train_loss= 100948.3374, test_loss= 83372.5139, time= 16.22sec, best_epoch=  137\n",
      "epoch= 150, train_loss= 100740.1765, test_loss= 83422.4437, time= 16.19sec, best_epoch=  147\n",
      "epoch= 160, train_loss= 100545.4952, test_loss= 82964.2857, time= 16.24sec, best_epoch=  158\n",
      "epoch= 170, train_loss= 99913.0836, test_loss= 82820.8311, time= 16.27sec, best_epoch=  168\n",
      "epoch= 180, train_loss= 99618.8953, test_loss= 82593.6836, time= 16.21sec, best_epoch=  179\n",
      "epoch= 190, train_loss= 98280.4206, test_loss= 82150.1207, time= 16.24sec, best_epoch=  187\n",
      "epoch= 200, train_loss= 98270.0785, test_loss= 81789.4880, time= 16.33sec, best_epoch=  198\n",
      "epoch= 210, train_loss= 97882.0718, test_loss= 81375.7186, time= 16.19sec, best_epoch=  210\n",
      "epoch= 220, train_loss= 97158.6377, test_loss= 80855.7902, time= 16.76sec, best_epoch=  219\n",
      "epoch= 230, train_loss= 95947.7133, test_loss= 79981.6600, time= 16.32sec, best_epoch=  230\n",
      "epoch= 240, train_loss= 94369.4213, test_loss= 78488.3114, time= 16.23sec, best_epoch=  240\n",
      "epoch= 250, train_loss= 91174.4163, test_loss= 75662.7583, time= 16.21sec, best_epoch=  250\n",
      "epoch= 260, train_loss= 84981.1273, test_loss= 69399.2266, time= 16.20sec, best_epoch=  260\n",
      "epoch= 270, train_loss= 69184.5126, test_loss= 53214.6082, time= 16.22sec, best_epoch=  270\n",
      "epoch= 280, train_loss= 40344.0249, test_loss= 24353.7255, time= 16.19sec, best_epoch=  280\n",
      "epoch= 290, train_loss= 25908.0384, test_loss= 7689.9316, time= 16.20sec, best_epoch=  290\n",
      "epoch= 300, train_loss= 22023.9761, test_loss= 4750.2142, time= 16.22sec, best_epoch=  300\n",
      "epoch= 310, train_loss= 20913.8000, test_loss= 3928.3503, time= 16.24sec, best_epoch=  310\n",
      "epoch= 320, train_loss= 19443.3049, test_loss= 3402.8154, time= 16.22sec, best_epoch=  320\n",
      "epoch= 330, train_loss= 18850.0072, test_loss= 2998.5732, time= 16.27sec, best_epoch=  328\n",
      "epoch= 340, train_loss= 18155.6613, test_loss= 2465.1344, time= 16.23sec, best_epoch=  340\n",
      "epoch= 350, train_loss= 17540.3106, test_loss= 2367.5463, time= 16.24sec, best_epoch=  348\n",
      "epoch= 360, train_loss= 16904.9863, test_loss= 2131.8829, time= 16.16sec, best_epoch=  357\n",
      "epoch= 370, train_loss= 16705.6483, test_loss= 1770.4870, time= 16.15sec, best_epoch=  370\n",
      "epoch= 380, train_loss= 15936.8941, test_loss= 1553.3946, time= 16.19sec, best_epoch=  380\n",
      "epoch= 390, train_loss= 15165.3086, test_loss= 1536.8510, time= 16.17sec, best_epoch=  387\n",
      "best_test_loss= 1396.3951, best_epoch=  397, time= 678.38sec\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEDCAYAAAAlRP8qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAif0lEQVR4nO3deZgc913n8fe3qo85NTpmJB+SI9mRnSg+EiNMDj9e4yQgO2DDw2U/hGNj4t0FB3gCAeeBNUl4lmcJx0LAIYhchCV2nMAGEZyY4Bicy4eMj1hybMuybEmxrdHomrO7q+q7f1TNqDWeoyXNTF+f1/P003VN1Vc1o0//+lfdvzJ3R0REml9Q7wJERGRhKNBFRFqEAl1EpEUo0EVEWoQCXUSkRSjQRURaRF0D3cw+aWYHzOyJGrf/aTPbaWY7zOyzi12fiEgzsXp+Dt3MrgBGgM+4+4XzbLsRuBO4yt0Pm9lqdz+wFHWKiDSDurbQ3f0+4FD1MjM7z8y+YmYPm9nXzew12ap3A7e5++HsZxXmIiJVGrEPfSvwHnf/PuA3gY9my88Hzjezb5rZ/Wa2pW4Viog0oFy9C6hmZj3Am4HPm9nk4mL2nAM2AlcCa4H7zOwidz+yxGWKiDSkhgp00ncMR9z99TOs2wc84O4V4Dkze5o04B9awvpERBpWQ3W5uPsx0rD+KQBLXZKt/iJp6xwz6yftgtldhzJFRBpSvT+2eDvwbeACM9tnZjcCPwvcaGaPATuA67LN7waGzGwncC/wPncfqkfdIiKNqK4fWxQRkYXTUF0uIiJy6up2UbS/v9/Xr19fr8OLiDSlhx9++KC7D8y0rm6Bvn79erZv316vw4uINCUze362depyERFpEQp0EZEWoUAXEWkRCnQRkRahQBcRaREKdBGRFqFAFxFpEU0X6C8dneCD/7yDSpzUuxQRkYbSdIH+6N4jfOqbe/jIPc/UuxQRkYbSdIG+5cIzeMfFZ/KJbzxHkmhgMRGRSU0X6ABvOa+fsXLM946O17sUEZGG0ZSBft5ANwDPDo7WuRIRkcbRnIG+ugeAZw+M1LkSEZHG0ZSBvqro9HXmeXZQgS4iMqn5An3nP2F/dB6v7xtl/xH1oYuITGq+QF/9OiiP8Fb/NsMTUb2rERFpGM0X6P2vhjMu4vKJ+xieqNS7GhGRhtF8gQ5w7pWcU97F8LgCXURk0ryBbmafNLMDZvbELOt/1sweN7PvmNm3zOyShS9zmt4zyXkFKx1d9EOJiDSLWlronwa2zLH+OeC/uPtFwO8DWxegrrn1rAGgszxErG+LiogANQS6u98HHJpj/bfc/XA2ez+wdoFqm13PagBW2xFGSrowKiICC9+HfiPw5dlWmtlNZrbdzLYPDg6e+lF6zgBggKO6MCoiklmwQDezHyQN9N+ebRt33+rum91988DAwKkfLGuhD9hhfXRRRCSTW4idmNnFwMeBq919aCH2OaeOPpKgwIAdVaCLiGROu4VuZucA/wj8nLs/ffol1XRQoq7VDNgRdbmIiGTmbaGb2e3AlUC/me0Dfg/IA7j7x4BbgVXAR80MIHL3zYtV8KSkcyUrjo5wTC10ERGghkB39xvmWf9LwC8tWEU1CorddNkR9quFLiICNOs3RYGw2E0nJUZKcb1LERFpCE0b6EGhmy5KlCIFuogINHGgW6GLLitRipJ6lyIi0hCaNtDJd9FJiVJFgS4iAs0c6FMtdHW5iIhAMwd6vpsOypQr+tiiiAg0c6AXugBIKmN1LkREpDE0b6Dn00CnNFrfOkREGkTzB3qkG0WLiEAzB3rW5WJldbmIiEAzB3q+GwCLFOgiItDMgZ610EN1uYiIAM0c6FkfehirhS4iAq0Q6Gqhi4gAzRzoWZdLPlagi4hAMwd6dlE0l0zUuRARkcbQxIHeCUAuVqCLiEAzB3quCECYVHD3OhcjIlJ/zRvoQUhCSMEqGhNdRIRmDnQgDvIUiDQmuogITR7oSZCnQEVjoouI0OSB7mEhC3S10EVE5g10M/ukmR0wsydmWW9m9hEz22Vmj5vZpQtf5sySoEDRIrXQRUSorYX+aWDLHOuvBjZmj5uAvzr9smrjYZE8ERPqQxcRmT/Q3f0+4NAcm1wHfMZT9wPLzezMhSpwztrCQnpRVF0uIiIL0od+NrC3an5ftuwVzOwmM9tuZtsHBwdP/8hZH3olVqCLiCzpRVF33+rum91988DAwOnvL1dUoIuIZBYi0PcD66rm12bLFl9YoGCRAl1EhIUJ9G3Az2efdnkjcNTdX1yA/c7LshZ6OdJX/0VEcvNtYGa3A1cC/Wa2D/g9IA/g7h8D7gKuAXYBY8B/XaxiXyFXpIha6CIiUEOgu/sN86x34FcWrKKTYLooKiIypam/KWr5IgW10EVEgGYP9LBIwSqUY/Whi4g0daAHky10fbFIRKT5Az2vLhcREaDZA11fLBIRmTLvp1waWZAvElpMWaMtiog0dwvdsvuKJlGpzpWIiNRfUwc6YRroXlagi4g0d6BnLXRXC11EpMkDPSwA6nIREYFmD3S10EVEpjR3oGctdGIFuohIcwf6ZAu9okAXEWnuQJ9qoZfrW4eISANoiUB3BbqISGsEukUKdBGR5g70XNblklTqW4eISANo7kCfbKGry0VEpEUCPVGgi4i0RqDH6nIREWmNQFcfuohIawR6oC4XEZHaAt3MtpjZU2a2y8xumWH9OWZ2r5k9YmaPm9k1C1/qDMI8AIFa6CIi8we6mYXAbcDVwCbgBjPbNG2z3wXudPc3ANcDH13oQmeUffVfgS4iUlsL/TJgl7vvdvcycAdw3bRtHFiWTfcB31u4EueQdbmE6nIREakp0M8G9lbN78uWVfsA8E4z2wfcBbxnph2Z2U1mtt3Mtg8ODp5CudMEIQkBoauFLiKyUBdFbwA+7e5rgWuAvzOzV+zb3be6+2Z33zwwMLAgB46DPIFHuPuC7E9EpFnVEuj7gXVV82uzZdVuBO4EcPdvAx1A/0IUOJ/E8hSpUIkV6CLS3moJ9IeAjWa2wcwKpBc9t03b5gXgrQBm9lrSQF+APpX5JUGePBGVOFmKw4mINKx5A93dI+Bm4G7gSdJPs+wwsw+Z2bXZZr8BvNvMHgNuB37Rl6gPJAkK5IkV6CLS9nK1bOTud5Fe7KxedmvV9E7gLQtbWm2SIE/eIsoKdBFpc839TVHAgzwF9aGLiLRAoId5CkRUIrXQRaS9tUCgF8mjLhcRkeYP9CBtoZfVQheRNtf0gU5YIG/62KKISNMHuoeFtA9dF0VFpM01faDbVKCrhS4i7a3pA51cQRdFRURogUC3XPopF31sUUTaXQsE+uRFUfWhi0h7a/5ADwvZN0XVQheR9tb8gZ5PL4qqD11E2l1Ng3M1siAsEmq0RRGRFgj0fIE8FV0UFZG21/RdLkGuSM4SoiiqdykiInXV9IEe5osARFG5zpWIiNRX0wd6kCsAkFRKda5ERKS+WiDQ0xZ6EinQRaS9NX2gM9lCV5eLiLS55g/0MA10V5eLiLS5lgl0tdBFpN21TKB7rEAXkfZWU6Cb2RYze8rMdpnZLbNs89NmttPMdpjZZxe2zDlkgY5a6CLS5ub9pqiZhcBtwNuBfcBDZrbN3XdWbbMReD/wFnc/bGarF6vgVwjzAHisPnQRaW+1tNAvA3a5+253LwN3ANdN2+bdwG3ufhjA3Q8sbJlzmOxyUQtdRNpcLYF+NrC3an5ftqza+cD5ZvZNM7vfzLbMtCMzu8nMtpvZ9sHBwVOreLrsc+ioD11E2txCXRTNARuBK4EbgL8xs+XTN3L3re6+2d03DwwMLMyRsy4X9aGLSLurJdD3A+uq5tdmy6rtA7a5e8XdnwOeJg34xZd1uViiQBeR9lZLoD8EbDSzDWZWAK4Htk3b5oukrXPMrJ+0C2b3wpU5hzDrcokqS3I4EZFGNW+gu3sE3AzcDTwJ3OnuO8zsQ2Z2bbbZ3cCQme0E7gXe5+5Di1X0CSa7XNSHLiJtrqYbXLj7XcBd05bdWjXtwHuzx9JSl4uICNAK3xTNPuVisbpcRKS9NX+gT3a5qIUuIm2uBQI97XIJ1IcuIm2udQI9UZeLiLS35g90M2LLEXiF9NqsiEh7av5ABxLLUyCiFCX1LkVEpG5aItDjIE9egS4iba4lAj0JCxSIKCvQRaSNtUSge5CnYBGlKK53KSIiddMyga4uFxFpd60R6GGRvLpcRKTNtUSgE+YpUFELXUTaWosEenpRtFRRH7qItK/WCXRTH7qItLfWCPR8Fx2U1YcuIm2tRQK9k05KaqGLSFtriUC3QlcW6OpDF5H21TqBbupyEZH21hKBHhTSPnR1uYhIO6vpnqKNLix2kadEqRLVuxQRkbppkUDvJrSESll3LRKR9lVTl4uZbTGzp8xsl5ndMsd2P2FmbmabF67E+QWFLgDi8uhSHlZEpKHMG+hmFgK3AVcDm4AbzGzTDNv1Ar8GPLDQRc7H8mmgV8YV6CLSvmppoV8G7HL33e5eBu4Arpthu98H/hCYWMD6apMFejQxsuSHFhFpFLUE+tnA3qr5fdmyKWZ2KbDO3f9lAWurXb4TgLg8VpfDi4g0gtP+2KKZBcCfAr9Rw7Y3mdl2M9s+ODh4uoc+brKFXlKgi0j7qiXQ9wPrqubXZssm9QIXAv9uZnuANwLbZrow6u5b3X2zu28eGBg49aqny1roXlIfuoi0r1oC/SFgo5ltMLMCcD2wbXKlux919353X+/u64H7gWvdffuiVDyTyUCvjC/ZIUVEGs28ge7uEXAzcDfwJHCnu+8wsw+Z2bWLXWBNsi4XFOgi0sZq+mKRu98F3DVt2a2zbHvl6Zd1kgqTga4+dBFpXy0xlstkCz2MFegi0r5aJNDTPvRcXCKKNUCXiLSn1gj0XBroXVZitKwx0UWkPbVGoAcBlVw3vYwxVtaIiyLSnloj0IFKoY8+G2G0pEAXkfbUMoEeF/roY5SRkrpcRKQ9tUygJx3LWW6jaqGLSNtqmUAPulbQxyiHx3STCxFpTy0T6PmeVSy3EYZGFOgi0p5aJtALPStZxihDw0s/HLuISCNomUAPulZQtIhjw8fqXYqISF20TKDTsRyAieFD9a1DRKROWifQO1cAUBkZqnMhIiL10UKBvhyAZEwtdBFpT60T6N2rAciPH6xzISIi9dE6gb7sTAB6K4OUI424KCLtp3UCvWM5UVDkDDvMi0d15yIRaT+tE+hmVLrXsMYOs++wAl1E2k/rBDpgvWexxg6z95DuXCQi7aelAr2w4mzOsEPsPaxAF5H201KBHvSdxRl2mL1DCnQRaT8tFej0raNIhWMH99e7EhGRJVdToJvZFjN7ysx2mdktM6x/r5ntNLPHzeweM3vVwpdag/6NACSDT+lm0SLSduYNdDMLgduAq4FNwA1mtmnaZo8Am939YuALwIcXutCa9J8PwNrke+waHKlLCSIi9VJLC/0yYJe773b3MnAHcF31Bu5+r7tPdlzfD6xd2DJr1HsWSa6L8+x7fGff0bqUICJSL7UE+tnA3qr5fdmy2dwIfHmmFWZ2k5ltN7Ptg4ODtVdZqyDABjZyQfg9Hn7+8MLvX0SkgS3oRVEzeyewGfijmda7+1Z33+zumwcGBhby0MdrWHMhl+T2cM+TL5MkvijHEBFpRLUE+n5gXdX82mzZCczsbcDvANe6e2lhyjsF636A3vgovaN7eGSvWuki0j5qCfSHgI1mtsHMCsD1wLbqDczsDcBfk4b5gYUv8ySc8yYA3lLcxd/c91xdSxERWUrzBrq7R8DNwN3Ak8Cd7r7DzD5kZtdmm/0R0AN83sweNbNts+xu8fVvhO4Bfm7VLr6y4yX1pYtI2zD3+vQzb9682bdv3744O//Se/FHP8tbg4+T6+zlH3/5LfQUc4tzLBGRJWRmD7v75pnWtdY3RSdd+BNYNM7HLt3Ls4Oj/OrtjxDrAqmItLjWDPRXvRnWXMT5z3yCD/7oa/jadw/wntv/k4lKXO/KREQWTWsGuhn8l9+CoWd4Z/Ilfvcdr+Wu77zEz2y9n10H9A1SEWlNrRnoAK/9UXjNj8A9H+SXzniWj73zUvYcHOWaj3ydv7jnGZ4fGq13hSIiC6p1A90MfuyvYPUmuPPn2bLseb763iu46oLV/MlXn+bKP/53fu4TD3Df04vwjVURkTpozU+5VBs5AJ+6Go59D67+MLzhnewaHOWLj+zni4/uZ9/hcc5Z2cVFa/v4xTevp7+nyIb+7sWvS0TkFMz1KZfWD3SA4ZfhH26EPV+H866Ct/4enPV6Jioxf//ACzz03CG+uesgw6UIgFXdBTau6eGy9Ssp5AIuXrucDf3drF3RiZktTc0iIjNQoAMkCTz41/Affwjjh9M+9u9/N2y4Asw4PFrmX3e+xMGRMs8dHOWJ/Uf57kvDJ+wiHxq9HXkuPWc5ceKEgbGqu0h/byF7LtLfXSBxiN05s6+DwOCs5Z1TH5vsKuQYHC6xsrtAIde6PV4isjgU6NUmjsK3/hIe3AoTR2DluenF040/BOe8EcL81KaVOKESJzyw+xD7j4yz7/A4QyMlHn7hMF2FkDiBoZESQ6Plk/6cuxms6e3A8RNCPhcaucAwjGMTFaLE6Snm6C6GFMKAQi6gIx/S15lntBQxVo7pLuZY3pknFxqHRsss7ywQBLCiq8BYOaarEOJAnDgru9NlcZKwZlkHAMfGK/R15inHTiVOyAVGIRfQU8wxWoqIE6cjH9KRDynkAgJLawxne1haR0c+ZM2yIgCHxyoUcwHdxRy5wBgaLZMPjWIuoBCGhKERGExUEvp7ChwerRCGxqruAh35kFIUM1FOWNaZ49BomcTTF9gwMHJBcPy86R2UtDgF+kwq47Dzn+DRv4fnvw1JBQq9cObFcMZFcOYlsOZCWPEq6Oibc1dJ4hwdr3BwpMTBkTIAgcG+w+PEiXN4rExghuOMlGIGegocHCmz/8g4AOPlmIHeIoMj6ZhmlSjBDHo78uRDY6QUM1qKKEcJ5ShhrBJxdLxCTzFPdyFkpJTOV+KEFV0FjoxXcHcOjZbpLuYYK8cYEARGOdt3YNY0X7bqKoSMV2LcIQzmrnvyRSUfGGf0dVCJnShOMEuXBwa5MH2xcneK+ZA4SbcJAqOnmKMzWzZajugu5Ogq5ujKXlTCIKAUxeQCm1o+XokZnohY2V1geCKis5C+SPV05FjVXaC3I0eUOKOliMRhWUeOZZ35qRfG/p70XV4YGKOlmMBgeVeBnmKOMNALlJxIgT6f0jDs/g/YfS+8+Di8vAMqVR9rLC6DvrXHH50rodibPZZVTfdCoRuCHOQ6IN+ZzYd1+We5O5YF92QujJZjOnIBldgZLlVwh96OHCOliGIYks+loV+JnWMTFXo7cuSCgIlKzHglphInxImTJBAlCYk7UezE7sTJ8UdfZ55SlHBgeAJIA6oSJYyVY0pRzKruIlHilOP0RSpJ0n0UwoCDIyV6O/IEBkOjZYZGyizrzNFVCDk6XqG/p0guDIiyWqIskKPs2JXYKUUxLx6ZoCMfkA8DYnfcydYnHJuoEJhl70gCwsBI3Dk2ETFWisiFAd2FkOGJiPFKPPVOpxTFdORD3GGsnL5D6siH9BRzvHR0gr6uPFGcnr/RcsTp/PfqzIf0duTIhwHFXEBPR46Nq3sZ6C0y0Fvk3P5uzh3oZu2KLgV/G1Ggn6wkhqFn4cAOOLoPjuxNn49mzxNHwE/inqVhAYJ8GuwWpM9BDiysWpbLprNl1dOWbR8EMy/D0n2YnTht09bNu36G7aeW2QzHmWk9p/Az06eDE6dPqGtyX5Cm5eTfr6XnIsilD6oCbvLngmDa/mr599nxfQa5qn1lv4Oq32cc5Alzxez3HRAnzpGxMscmInJZ698MhifSd1Rx9iI2OFzi4EiJJHG6iznKUfqC89LREuOViHKUvvAdGi2xe3CUgyMlKvHx/7eFMGB9fxevWpVeuH/dWX1c/up+1iwrqguqBc0V6BqxaiZBCAPnp4+ZuKddNqXh7HHs+HR5NO2+iSbSbcpjUBmDJEpfKDxOn5Mom06y5+r1syyLysd/fmo/MeBpTZ7MMJ1k8z5tvnp9UrV+2s9X73/6vuUEJ7wPC/KEYYFVuQKrwgKERcgVICywPCywLixArphes6laR5gtyxWhWICuQvrOr/dM6D0DetfhvWs4VCny3NAYzw6OsHtwlGcHR3h+aJRvPHOQ8coeAM4d6OYnLl3Lj7/hbM5a3lmPUyJLTC10OT3uM7wgVL2wzLlsrvXJtOmqFyZPjrfSJ1vSnhx/oYsr1QVW1Zgc327WY81Q1+SLa1zJlk2+qCac8MIalyAupy+8cdUjKqU/G5dOnI4r2fwM21Xvx2cYgyjfnQV8FvR9a+HMi0nWXMKOiVU89PwRvvLESzy45xCBwQ9esJp3Xb6BN5+3Sq32JqcuF5FmVhpOv0sx/CIMvzTz89F96TtDgM4VcP4WeM072LviTXzusSHueGgvB0dKXLy2j19/20aues2a+v6b5JQp0EVaXVSGwSfhxcdgzzfh6a+k13pyHXD+Fkrf/9/4h5fP5q+/vpvnh8b4wQsG+J8/solzB3rqXbmcJAW6SLuJK/DCt+HJL8Hjn0vD/ezNVN70q/ztoQv5s3t2UYpi3nX5Bt5z1UbdAKaJKNBF2ll5FB79LNz/UTi0G9b9AIcuv5U/eLyXLzy8j9W9Rd73wxfwk9+3Vv3rTaD97lgkIscVuuGyd8PN2+Hav4DDe1h5+zv44/A2/vkXz2Ptik7e94XHefdnHubepw40zRfO5JXUQhdpN6UR+Mafwrf+AoI8yRXv4xPRFv78319gpBTx6tU9XHfJWfzQ687g/DU9arU3GHW5iMgrHdoNd/8uPPUvsPJcSm/7A+6uXMKnvvkcj7xwBICzl3dy0dl9rOopsOmsZXQVQs5Z2U1gsHpZByu68hTC9Ju2Cv6loUAXkdnt+jf48i0w9Ay8+u1wxfs40HcxX/3uAb7xzEGeemmYweHS1PDSMzFLv7GaDwM68gErugpUsvFx8pODp4UB+cDIhUY+DOgu5FjWmaOSDQqXDwOixNNhDoo5eoo5OvIBE5X0W9kd+XRwusmhGoJsILjAmJqeXB5k4xUFNjmA2/HnIDBKUUIxFxDFjhl0F3N0F0KOTUSs7i0yPBGdMODcKwajMyMMjx9zcn31i9pIKaIzH1KJEzryCzf8x2kHupltAf6c9MtwH3f3/z1tfRH4DPB9wBDwM+6+Z659KtBFGkhUTkcg/Y8PQ+koDLwWXvdj6fDSZ72BOOxgz9Ao7vDCofT55WMljk1UqERJOiZPnFCJnLFyOrRBPgxIPA3rKHYqk2PuxE4lSTg2XmG0FJPPpaFfzkb6LEcJI6WIkWwws8lhapqhaz8wskHg0heNSZ35kMR96gXmXZdv4L1vn+Wb6PM4rUA3sxB4Gng7sA94CLjB3XdWbfPLwMXu/t/N7Hrgx939Z+barwJdpAGVRuA7d8Ljn08/9oinY9is2ADLzkq/kdq1EnKd6eBz+a5pzx0njlFUPd6N2bR10x92wrxjVBLI59LtK4lRip3EjdiNmPQ5AeLESKbmDcemBoxLErLp9MUkTpxCLqAcJVODmo2Wo3SkzHzIgeES/T3pfQ3SAd8S4oQTnqOqgehid+KqAeqiqgHqJioxxVzAkbEKYWA4EMXOm89bxds2ndqXu053LJfLgF3uvjvb2R3AdcDOqm2uAz6QTX8B+EszM69Xf46InJpiD2x+V/oYHYJ9D8LeB9L+9qP74dmvpfcUqIwteikGFKrmC9Pm59/B9MHYpr+AZEepHoyt+hmmLWOWaZuarXnbVb8AvOdk/jU1qSXQzwb2Vs3vA35gtm3cPTKzo8Aq4GD1RmZ2E3ATwDnnnHOKJYvIkuheBRdcnT6mcz8+AN3UYyx99rhqbJ1svBv342PtVK+baaydVzymDSo302P6wHO17mf6+EJTz5y4bPLfPDXNSWw7w357FmfohSX9epi7bwW2QtrlspTHFpEFZJZ1sWgUx0ZSyxeL9gPrqubXZstm3MbMckAf6cVRERFZIrUE+kPARjPbYGYF4Hpg27RttgG/kE3/JPA19Z+LiCytebtcsj7xm4G7ST+2+El332FmHwK2u/s24BPA35nZLuAQaeiLiMgSqqkP3d3vAu6atuzWqukJ4KcWtjQRETkZGpxLRKRFKNBFRFqEAl1EpEUo0EVEWkTdRls0s0Hg+VP88X6mfQu1gTRqbarr5Kiuk6O6Tt6p1vYqdx+YaUXdAv10mNn22QanqbdGrU11nRzVdXJU18lbjNrU5SIi0iIU6CIiLaJZA31rvQuYQ6PWprpOjuo6Oarr5C14bU3Zhy4iIq/UrC10ERGZRoEuItIimi7QzWyLmT1lZrvM7JY617LHzL5jZo+a2fZs2Uoz+6qZPZM9r1iCOj5pZgfM7ImqZTPWYamPZOfvcTO7dInr+oCZ7c/O2aNmdk3VuvdndT1lZj+8iHWtM7N7zWynme0ws1/Lltf1nM1RVyOcsw4ze9DMHstq+2C2fIOZPZDV8LlsiG3MrJjN78rWr1/iuj5tZs9VnbPXZ8uX7O8/O15oZo+Y2Zey+cU9X+7eNA/S4XufBc4lvb3gY8CmOtazB+iftuzDwC3Z9C3AHy5BHVcAlwJPzFcHcA3wZdKbG74ReGCJ6/oA8JszbLsp+30WgQ3Z7zlcpLrOBC7NpntJb4K+qd7nbI66GuGcGdCTTeeBB7JzcSdwfbb8Y8D/yKZ/GfhYNn098LklruvTwE/OsP2S/f1nx3sv8FngS9n8op6vZmuhT92w2t3LwOQNqxvJdcDfZtN/C/zYYh/Q3e8jHYe+ljquAz7jqfuB5WZ25hLWNZvrgDvcveTuzwG7SH/fi1HXi+7+n9n0MPAk6X1x63rO5qhrNkt5ztzdR7LZfPZw4CrSG8PDK8/Z5Ln8AvBWs6k7Ji9FXbNZsr9/M1sLvAP4eDZvLPL5arZAn+mG1XP9wS82B/7VzB629AbYAGvc/cVs+iVgce4GO7/Z6miEc3hz9nb3k1VdUnWpK3tr+wbSll3DnLNpdUEDnLOs++BR4ADwVdJ3BEfcPZrh+CfcOB6YvHH8otfl7pPn7H9l5+z/mFlxel0z1LzQ/gz4LSDJ5lexyOer2QK90Vzu7pcCVwO/YmZXVK/09P1T3T8X2ih1ZP4KOA94PfAi8Cf1KsTMeoB/AH7d3Y9Vr6vnOZuhroY4Z+4eu/vrSe8rfBnwmnrUMd30uszsQuD9pPV9P7AS+O2lrMnMfgQ44O4PL+Vxmy3Qa7lh9ZJx9/3Z8wHg/5H+kb88+RYuez5Qp/Jmq6Ou59DdX87+AybA33C8i2BJ6zKzPGlo/r27/2O2uO7nbKa6GuWcTXL3I8C9wJtIuywm73xWffwlv3F8VV1bsu4rd/cS8CmW/py9BbjWzPaQdg1fBfw5i3y+mi3Qa7lh9ZIws24z652cBn4IeIITb5j9C8A/1aO+OerYBvx8drX/jcDRqm6GRTetv/LHSc/ZZF3XZ1f7NwAbgQcXqQYjvQ/uk+7+p1Wr6nrOZqurQc7ZgJktz6Y7gbeT9vHfS3pjeHjlOVv0G8fPUtd3q16YjbSfuvqcLfrv0t3f7+5r3X09aU59zd1/lsU+Xwt5RXcpHqRXqZ8m7b/7nTrWcS7pJwweA3ZM1kLa73UP8Azwb8DKJajldtK34hXSfrkbZ6uD9Or+bdn5+w6weYnr+rvsuI9nf8RnVm3/O1ldTwFXL2Jdl5N2pzwOPJo9rqn3OZujrkY4ZxcDj2Q1PAHcWvX/4EHSC7KfB4rZ8o5sfle2/twlrutr2Tl7Avi/HP8kzJL9/VfVeCXHP+WyqOdLX/0XEWkRzdblIiIis1Cgi4i0CAW6iEiLUKCLiLQIBbqISItQoIuItAgFuohIi/j/DtT1ChUqFW4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "multi_train_loss, multi_test_loss = singal_train_CNN(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0610c-c931-4e0d-b25b-844103d9fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7649e-1b0a-4083-bfaa-4d1517e4211a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
