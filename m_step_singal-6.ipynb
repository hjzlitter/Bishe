{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9dbe5b-c63e-4c20-b71d-ae06edb34869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "# import seaborn as sns\n",
    "from numpy import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import original_data\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "# config\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE =  256\n",
    "EPOCHS = 400\n",
    "N_FRAME = 12\n",
    "# TIME_LENGTH = 40\n",
    "BEFORE_LENGTH = 10 # 20 30\n",
    "AFTER_LENGTH = 10 # 20 30 \n",
    "N_SAMPLE = 10000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PATH = './data/timedt.data.400'\n",
    "\n",
    "def get_original_data(\n",
    "   path:str\n",
    "):\n",
    "    \"\"\"\n",
    "    input temperature : 温度\n",
    "    \n",
    "    return t, msd, csp, xyz, r_, v_xyz, v_, angle, g\n",
    "\n",
    "    t       时间序列(单位:ps)\n",
    "    msd     单He的msd(均方位移)\n",
    "    csp     CSP(中心对称参数)\n",
    "    xyz     单He的xyz坐标\n",
    "    r_      单He离原点距离\n",
    "    v_xyz   单He的沿xyz坐标的速度分量\n",
    "    v_      单He的速度大小\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as fin:\n",
    "        t = []      # 时间序列(单位:ps)\n",
    "        msd = []    # 单He的msd(均方位移)\n",
    "        csp = []    # CSP(中心对称参数)\n",
    "        xyz = []    # 单He的xyz坐标\n",
    "        r_ = []     # 单He离原点距离\n",
    "        v_xyz = []  # 单He的沿xyz坐标的速度分量\n",
    "        v_ = []     # 单He的速度大小\n",
    "        for i, line in enumerate(fin.readlines()[1:]):\n",
    "            data = list(map(float, line.strip().split(' ')))\n",
    "            t.append(data[0])\n",
    "            msd.append(data[1])\n",
    "            csp.append(data[2:8])\n",
    "            xyz.append(data[8:11])\n",
    "            r_.append(data[11])\n",
    "            v_xyz.append(data[12:15])\n",
    "            v_.append(data[15])\n",
    "\n",
    "    # with open(G_PATH.format(Temperature=temperature), 'r', encoding='utf-8') as fin:\n",
    "    #     g = []      # g参数\n",
    "    #     for i, line in enumerate(fin.readlines()[1:]):\n",
    "    #         data = list(map(float, line.strip().split(' ')))\n",
    "    #         g.append(data[1:7])\n",
    "    # indices_to_remove = np.arange(1001, len(t) - 1, 1001)\n",
    "    t = np.array(t)\n",
    "    # t = np.delete(t, indices_to_remove)\n",
    "    t = t.reshape(-1, 1)\n",
    "    msd = np.array(msd)\n",
    "    # msd = np.delete(msd, indices_to_remove)\n",
    "    msd = msd.reshape(-1, 1)\n",
    "    csp = np.array(csp)\n",
    "    # csp = np.delete(csp, indices_to_remove, axis=0)\n",
    "    xyz = np.array(xyz)\n",
    "    # xyz = np.delete(xyz, indices_to_remove, axis=0)\n",
    "    r_ = np.sqrt(np.array(r_))\n",
    "   #  r_ = np.delete(r_, indices_to_remove)\n",
    "    r_ = r_.reshape(-1, 1)\n",
    "    v_xyz = np.array(v_xyz)\n",
    "    # v_xyz = np.delete(v_xyz, indices_to_remove, axis=0)\n",
    "    v_ = np.sqrt(np.array(v_))\n",
    "    # v_ = np.delete(v_, indices_to_remove)\n",
    "    v_ = v_.reshape(-1, 1)\n",
    "    angle = np.arccos(v_xyz / v_.reshape(len(t), 1))\n",
    "    # g = np.array(g)\n",
    "\n",
    "    return t, msd, csp, xyz, r_, v_xyz, v_, angle# , g\n",
    "def get_iter(path, batch_size):\n",
    "    '''\n",
    "    这个index有没有用另外说\n",
    "    '''\n",
    "    data = get_original_data(PATH)\n",
    "    # index = np.where(label[TIME_LENGTH//2 : -TIME_LENGTH//2])[0] + TIME_LENGTH//2\n",
    "    # data_num = int(len(data[0])/TIME_LENGTH)\n",
    "    data_num = int(len(data[0])/(BEFORE_LENGTH + AFTER_LENGTH))\n",
    "    # diff_xyz = np.diff(data[3], axis=0)\n",
    "    # diff_xyz = np.vstack(([0, 0, 0], diff_xyz))\n",
    "    # y = diff_xyz[:, 0].reshape(-1, 1)\n",
    "    y = data[3][:,2].reshape(-1,1)\n",
    "    data = np.hstack(data[1:])\n",
    "    \n",
    "    # data = data[3]\n",
    "    INPUT_SIZE = data.shape[-1]\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    data = scaler.fit_transform(data)\n",
    "    # y = scaler.fit_transform(y)\n",
    "    \n",
    "    data_X = torch.from_numpy(data)\n",
    "    # print(data_X.shape)\n",
    "    data_Y = torch.from_numpy(y)\n",
    "    data_Y = data_Y.squeeze(1)\n",
    "\n",
    "    X = torch.zeros(data_num, BEFORE_LENGTH, INPUT_SIZE)\n",
    "    # Y = torch.zeros(data_num, TIME_LENGTH//2)\n",
    "    Y = torch.zeros(data_num, 1)\n",
    "    i=0\n",
    "        # for i, idx in enumerate(sample_indices):\n",
    "    # for idx in range(TIME_LENGTH//2, len(data)-TIME_LENGTH//2, TIME_LENGTH):\n",
    "    #     X[i, :, :] = data_X[idx-TIME_LENGTH//2 : idx]\n",
    "    #     Y[i, :]    = data_Y[idx+TIME_LENGTH//2]# -data_Y[idx]\n",
    "    #     i+=1\n",
    "    \n",
    "    for idx in range(BEFORE_LENGTH, len(data)-AFTER_LENGTH, (BEFORE_LENGTH+AFTER_LENGTH)):\n",
    "        #print(idx-BEFORE_LENGTH)\n",
    "        X[i, :, :] = data_X[idx-BEFORE_LENGTH : idx]\n",
    "        \n",
    "        Y[i, :]    = data_Y[idx+AFTER_LENGTH] -data_Y[idx]\n",
    "        i+=1\n",
    "    Y = F.normalize(Y, p=2, dim=1)\n",
    "    X = X.float().unsqueeze(1) # [Batch_size, C, H, W]\n",
    "    shuffled_index = np.random.permutation(range(data_num))\n",
    "    X = X[shuffled_index]\n",
    "    Y = Y[shuffled_index]\n",
    "    Y = Y.view(Y.size(0), -1)\n",
    "    Train_X, Test_X = X[:int(data_num*0.8)], X[int(data_num*0.8):]\n",
    "    Train_Y, Test_Y = Y[:int(data_num*0.8)], Y[int(data_num*0.8):]\n",
    "\n",
    "\n",
    "    Train_generator = DataLoader(\n",
    "        torch.utils.data.TensorDataset(Train_X, Train_Y), \n",
    "        batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    Test_generator = DataLoader(\n",
    "        torch.utils.data.TensorDataset(Test_X, Test_Y), \n",
    "        batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    return Train_generator, Test_generator\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        Channel_in, \n",
    "        Height_in, \n",
    "        Width_in,\n",
    "        Embedding_list,\n",
    "        Output_size, \n",
    "        Filter_num, \n",
    "        Kernel_list, \n",
    "        dropout = 0.5, \n",
    "    ):\n",
    "        super(CNN, self).__init__()\n",
    "        self.Embedding_list = Embedding_list\n",
    "        self.emb = nn.ModuleList([\n",
    "            nn.Sequential( nn.Linear(emb, Width_in),)\n",
    "         for emb in Embedding_list])\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    Channel_in, \n",
    "                    Filter_num, \n",
    "                    kernel_size=(kernel, Width_in), \n",
    "                    padding=((kernel - 1) // 2, 0), \n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(\n",
    "                    kernel_size=((Height_in+3)//4, 1), \n",
    "                    stride=(Height_in+3)//4, \n",
    "                    padding=((Height_in-Height_in//4*4+1)//2, 0), \n",
    "                ), \n",
    "            )\n",
    "            for kernel in Kernel_list\n",
    "        ])\n",
    "        # print(Kernel_list)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout), \n",
    "            nn.Linear(Filter_num * len(Kernel_list) * 4, 64),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, 16), \n",
    "            nn.Linear(16, Output_size)\n",
    "        )\n",
    "        # one -hot 时就这样就行 output_size=4\n",
    "        # self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_list = []\n",
    "        temp = 0\n",
    "        for emb in self.Embedding_list:\n",
    "            x_list.append(x[:,:,:,temp:temp+emb])\n",
    "            temp = temp+emb\n",
    "        embx = []\n",
    "        for i, emb in enumerate(self.emb):\n",
    "            embx.append(emb(x_list[i]))\n",
    "        x = sum(tensor for tensor in embx)\n",
    "        out = [conv(x) for conv in self.convs]\n",
    "        out = torch.cat(out, dim=1)\n",
    "        out = out.view(x.size(0), -1)\n",
    "        # print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        # output = self.output(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def train(\n",
    "    net:nn.Module, \n",
    "    Train_generator, \n",
    "    loss_func,\n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    device\n",
    "):\n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "    sum_loss = []\n",
    "    \n",
    "    Train_generator =  DataLoader(Train_generator.dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "    for x, y in Train_generator:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        scores = net(x)\n",
    "        loss = loss_func(scores, y)\n",
    "        sum_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return np.array(sum_loss).mean()\n",
    "# def train(net, Train_generator, loss_func, optimizer, scaler, scheduler, device):\n",
    "#     net = net.to(device)\n",
    "#     net.train()\n",
    "#     sum_loss = []\n",
    "    \n",
    "#     Train_generator = DataLoader(Train_generator.dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "    \n",
    "#     for x, y in Train_generator:\n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # 使用autocast启用半精度计算\n",
    "#         with autocast():\n",
    "#             scores = net(x)\n",
    "#             loss = loss_func(scores, y)\n",
    "        \n",
    "#         # 放大损失值以保持数值稳定性\n",
    "#         scaler.scale(loss).backward()\n",
    "        \n",
    "#         # 应用优化器和梯度缩放\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "        \n",
    "#         # 清零梯度，为下一次迭代做准备\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # 将损失值记录下来\n",
    "#         sum_loss.append(loss.item())\n",
    "    \n",
    "#     # 学习率调度器步骤\n",
    "#     scheduler.step()\n",
    "    \n",
    "#     return np.array(sum_loss).mean()\n",
    "\n",
    "def evaluate(\n",
    "    net:nn.Module, \n",
    "    Test_generator, \n",
    "    loss_func,\n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    device\n",
    "):\n",
    "    sum_loss = []\n",
    "    all_errors = []  # 收集所有的误差百分比\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():  # 不计算梯度，减少内存和计算需求\n",
    "        for x, y in Test_generator:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            scores = net(x)\n",
    "            loss = loss_func(scores, y)\n",
    "            sum_loss.append(loss.item())\n",
    "\n",
    "            # 计算误差百分比\n",
    "            y_cpu = y.detach().cpu().numpy()\n",
    "            scores_cpu = scores.detach().cpu().numpy()\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                error_percentage = np.abs((scores_cpu - y_cpu) / y_cpu) * 100\n",
    "                error_percentage[np.isinf(error_percentage)] = 0  # 处理无穷大\n",
    "                error_percentage = np.nan_to_num(error_percentage)  # NaN 转为 0\n",
    "\n",
    "            all_errors.extend(error_percentage)  # 收集所有批次的误差\n",
    "\n",
    "    average_loss = np.mean(sum_loss)\n",
    "    average_error_percentage = np.mean(all_errors)  # 计算所有样本的平均误差百分比\n",
    "\n",
    "    return average_loss, average_error_percentage\n",
    "\n",
    "\n",
    "\n",
    "def train_multi_epochs(\n",
    "    path, \n",
    "    net:nn.Module, \n",
    "    Train_generator, \n",
    "    Test_generator,     \n",
    "    loss_func,\n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    epochs, \n",
    "    device, \n",
    "    information:str, \n",
    "    show_train_process=None\n",
    "):\n",
    "    best_epoch = 0\n",
    "    best_test_loss = 9.9e9\n",
    "    best_net = net.state_dict()\n",
    "    sum_train_loss, sum_test_loss, sum_per_error = [], [], []\n",
    "    date0 = time.strftime('%Y-%m-%d %a %H-%M-%S', time.localtime(time.time()))\n",
    "\n",
    "    with open(path.format(date=date0, information=information), 'w') as log_fin:\n",
    "        log_fin.write(information + '\\n')\n",
    "        log_fin.write('epoch' + ' ' + 'train_loss' + ' ' + 'test_loss' + ' ' + 'time' + ' ' + 'best_epoch' + '\\n')\n",
    "        t1 = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t0 = time.time()\n",
    "            train_loss = train(net, Train_generator, loss_func, optimizer, scheduler, device).item()\n",
    "            test_loss, per_error = evaluate(net, Test_generator, loss_func, optimizer, scheduler, device)#.item()\n",
    "\n",
    "            sum_train_loss.append(train_loss)\n",
    "            sum_test_loss.append(test_loss)\n",
    "            sum_per_error.append(test_loss)\n",
    "            if epoch == 0 or test_loss < best_test_loss:\n",
    "                best_epoch = epoch\n",
    "                best_test_loss = test_loss\n",
    "                best_net = net.state_dict()\n",
    "\n",
    "            log_fin.write(str(epoch) + ' ' + str(train_loss) + ' ' + str(test_loss) + ' ' + str(time.time()-t0) + ' ' + str(best_epoch) + ' '+str(per_error) + '\\n')\n",
    "            if show_train_process != None and epoch % show_train_process == 0:\n",
    "                print('epoch={:>4}, train_loss= {:.4f}, test_loss= {:.4f}, time= {:.2f}sec, best_epoch= {:>4}, per_error={:.4f}'.format(epoch, train_loss, test_loss, time.time()-t1, best_epoch, per_error))\n",
    "                t1 = time.time()\n",
    "        \n",
    "        log_fin.write('\\n')\n",
    "        log_fin.write('best_epoch=' + str(best_epoch) + '\\n')\n",
    "        log_fin.write('best_test_loss=' + str(best_test_loss) + '\\n')\n",
    "        log_fin.write('per_error=' + str(per_error) + '\\n')\n",
    "    return best_test_loss, best_epoch, best_net, sum_train_loss, sum_test_loss, sum_per_error\n",
    "\n",
    "# def train_multi_epochs(\n",
    "#     path, \n",
    "#     net:nn.Module, \n",
    "#     Train_generator, \n",
    "#     Test_generator,     \n",
    "#     loss_func,\n",
    "#     optimizer, \n",
    "#     scheduler,\n",
    "#     scaler,  # 添加scaler参数\n",
    "#     epochs, \n",
    "#     device, \n",
    "#     information:str, \n",
    "#     show_train_process=None\n",
    "# ):\n",
    "#     best_epoch = 0\n",
    "#     best_test_loss = 9.9e9\n",
    "#     best_net = net.state_dict()\n",
    "#     sum_train_loss, sum_test_loss = [], []\n",
    "#     date0 = time.strftime('%Y-%m-%d %a %H-%M-%S', time.localtime(time.time()))\n",
    "\n",
    "#     with open(path.format(date=date0, information=information), 'w') as log_fin:\n",
    "#         log_fin.write(information + '\\n')\n",
    "#         log_fin.write('epoch' + ' ' + 'train_loss' + ' ' + 'test_loss' + ' ' + 'time' + ' ' + 'best_epoch' + '\\n')\n",
    "#         t1 = time.time()\n",
    "#         for epoch in range(epochs):\n",
    "#             t0 = time.time()\n",
    "#             train_loss = train(net, Train_generator, loss_func, optimizer, scaler, scheduler, device).item()\n",
    "#             test_loss = evaluate(net, Test_generator, loss_func, optimizer, scheduler, device).item()\n",
    "\n",
    "#             sum_train_loss.append(train_loss)\n",
    "#             sum_test_loss.append(test_loss)\n",
    "\n",
    "#             if epoch == 0 or test_loss < best_test_loss:\n",
    "#                 best_epoch = epoch\n",
    "#                 best_test_loss = test_loss\n",
    "#                 best_net = net.state_dict()\n",
    "\n",
    "#             log_fin.write(str(epoch) + ' ' + str(train_loss) + ' ' + str(test_loss) + ' ' + str(time.time()-t0) + ' ' + str(best_epoch) + '\\n')\n",
    "#             if show_train_process != None and epoch % show_train_process == 0:\n",
    "#                 print('epoch={:>4}, train_loss= {:.4f}, test_loss= {:.4f}, time= {:.2f}sec, best_epoch= {:>4}'.format(epoch, train_loss, test_loss, time.time()-t1, best_epoch))\n",
    "#                 t1 = time.time()\n",
    "        \n",
    "#         log_fin.write('\\n')\n",
    "#         log_fin.write('best_epoch=' + str(best_epoch) + '\\n')\n",
    "#         log_fin.write('best_test_loss=' + str(best_test_loss) + '\\n')\n",
    "#     return best_test_loss, best_epoch, best_net, sum_train_loss, sum_test_loss\n",
    "\n",
    "\n",
    "def singal_train_CNN(i, lr=LEARNING_RATE, ga=0.5, dropout=0.5):\n",
    "    t1 = time.time()\n",
    "    # scaler = GradScaler()\n",
    "    train_iter, test_iter = get_iter(PATH, BATCH_SIZE)\n",
    "    # test_iter = DataLoader([final_data, final_labels], batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "    # criteon = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "    criteon = nn.SmoothL1Loss().to(DEVICE)\n",
    "    # net = CNN(1, TIME_LENGTH, INPUT_SIZE, 1, 32, [3, 5, 7, 9], dropout)\n",
    "    # net = CNN(1,TIME_LENGTH//2, 9, [1,6,4,4,3],TIME_LENGTH//2, 8, [9,7,5,3], dropout)\n",
    "    # net = CNN(1,TIME_LENGTH//2, 25, [1,6,4,4,3], 1, 8, [9,7,5,3], dropout)\n",
    "    net = CNN(1,BEFORE_LENGTH, 25, [1,6,4,4,3], 1, 8, [9,7,5,3], dropout)\n",
    "    # net = CNN(1,TIME_LENGTH//2, 25, [3], 1, 8, [9,7,5,3], dropout)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=0)\n",
    "    scheduler = MultiStepLR(optimizer, [int(EPOCHS*0.2), int(EPOCHS*0.4)], ga, last_epoch=-1)\n",
    "\n",
    "    best_test_loss, best_epoch, best_net, multi_train_loss, multi_test_loss,_ = train_multi_epochs(f'./output/{i}', net, train_iter, test_iter, criteon, optimizer, scheduler, EPOCHS, DEVICE, 'Temperature={}'.format(300), show_train_process=10)\n",
    "    torch.save(best_net, 'best_model_weiyi_Z.pth')\n",
    "    print('best_test_loss= {:.4f}, best_epoch= {:>4}, time= {:.2f}sec'.format(best_test_loss, best_epoch, time.time()-t1))\n",
    "\n",
    "    plt.plot(multi_train_loss)\n",
    "    plt.plot(multi_test_loss)\n",
    "    plt.savefig(f'./output/{i}.png')\n",
    "    plt.close()\n",
    "    return multi_train_loss, multi_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c4a3ab-00bd-435e-bb46-0ad7f14efab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_original_data(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404eaae2-3a04-4270-82ee-dd213bb72113",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack(data[1:])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit_transform(data)\n",
    "mean_scaled = scaler.mean_\n",
    "\n",
    "var_scaled = scaler.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7adfd831-6abe-4d55-9fe5-ab94cee5c088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.89295518e+03, 1.17942775e+00, 5.75954319e-01, 1.69234412e+00,\n",
       "       1.37599649e+00, 1.34694644e+00, 1.97403935e+00, 8.50576127e+00,\n",
       "       7.78344116e+00, 8.89985569e+00, 8.88802331e+00, 9.06355493e+00,\n",
       "       9.09185981e+00, 9.06728313e+00, 6.11193960e+00, 6.83298149e-01,\n",
       "       6.85452737e-01, 6.81930338e-01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_scaled**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9a30c-9744-4e60-9a94-b752cb998cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(1,TIME_LENGTH//2, 25, [1,6,4,4,3], 1, 8, [9,7,5,3], 0.5)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805dd87d-1fe7-4925-9238-d129110ca180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应该是前10步就能预测了。关键是预测多少步比较好？再算算40步的\n",
    "# 然后就是流程图\n",
    "# 然后就是转移矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f5d5a57-4780-40ad-b288-d81f5ee1d941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=   0, train_loss= 0.3622, test_loss= 0.2464, time= 1.21sec, best_epoch=    0, per_error=56.6086\n",
      "epoch=  10, train_loss= 0.1977, test_loss= 0.1723, time= 12.12sec, best_epoch=   10, per_error=42.4758\n",
      "epoch=  20, train_loss= 0.1600, test_loss= 0.1353, time= 12.10sec, best_epoch=   20, per_error=36.6089\n",
      "epoch=  30, train_loss= 0.1365, test_loss= 0.1258, time= 11.99sec, best_epoch=   29, per_error=33.8805\n",
      "epoch=  40, train_loss= 0.1235, test_loss= 0.1084, time= 12.19sec, best_epoch=   40, per_error=30.3465\n",
      "epoch=  50, train_loss= 0.1115, test_loss= 0.0990, time= 12.37sec, best_epoch=   49, per_error=27.5770\n",
      "epoch=  60, train_loss= 0.0996, test_loss= 0.0883, time= 12.24sec, best_epoch=   59, per_error=25.5212\n",
      "epoch=  70, train_loss= 0.0869, test_loss= 0.0767, time= 12.12sec, best_epoch=   69, per_error=22.9976\n",
      "epoch=  80, train_loss= 0.0780, test_loss= 0.0679, time= 12.31sec, best_epoch=   80, per_error=20.8679\n",
      "epoch=  90, train_loss= 0.0699, test_loss= 0.0627, time= 12.28sec, best_epoch=   90, per_error=18.5276\n",
      "epoch= 100, train_loss= 0.0643, test_loss= 0.0593, time= 12.14sec, best_epoch=   99, per_error=17.0498\n",
      "epoch= 110, train_loss= 0.0607, test_loss= 0.0580, time= 12.24sec, best_epoch=  109, per_error=15.2594\n",
      "epoch= 120, train_loss= 0.0573, test_loss= 0.0536, time= 12.11sec, best_epoch=  120, per_error=13.6671\n",
      "epoch= 130, train_loss= 0.0555, test_loss= 0.0522, time= 12.45sec, best_epoch=  130, per_error=12.8539\n",
      "epoch= 140, train_loss= 0.0541, test_loss= 0.0510, time= 12.51sec, best_epoch=  140, per_error=12.3455\n",
      "epoch= 150, train_loss= 0.0535, test_loss= 0.0518, time= 12.29sec, best_epoch=  149, per_error=11.4231\n",
      "epoch= 160, train_loss= 0.0524, test_loss= 0.0508, time= 12.17sec, best_epoch=  159, per_error=11.2793\n",
      "epoch= 170, train_loss= 0.0516, test_loss= 0.0491, time= 12.16sec, best_epoch=  169, per_error=10.7788\n",
      "epoch= 180, train_loss= 0.0489, test_loss= 0.0493, time= 12.26sec, best_epoch=  177, per_error=10.5944\n",
      "epoch= 190, train_loss= 0.0486, test_loss= 0.0512, time= 12.33sec, best_epoch=  184, per_error=10.3549\n",
      "epoch= 200, train_loss= 0.0485, test_loss= 0.0498, time= 12.44sec, best_epoch=  198, per_error=10.2668\n",
      "epoch= 210, train_loss= 0.0474, test_loss= 0.0473, time= 12.25sec, best_epoch=  207, per_error=10.0516\n",
      "epoch= 220, train_loss= 0.0467, test_loss= 0.0447, time= 12.10sec, best_epoch=  220, per_error=9.7939\n",
      "epoch= 230, train_loss= 0.0459, test_loss= 0.0467, time= 12.17sec, best_epoch=  228, per_error=9.7029\n",
      "epoch= 240, train_loss= 0.0451, test_loss= 0.0440, time= 12.01sec, best_epoch=  236, per_error=9.6353\n",
      "epoch= 250, train_loss= 0.0441, test_loss= 0.0438, time= 12.60sec, best_epoch=  249, per_error=9.5271\n",
      "epoch= 260, train_loss= 0.0442, test_loss= 0.0465, time= 12.23sec, best_epoch=  256, per_error=9.2628\n",
      "epoch= 270, train_loss= 0.0435, test_loss= 0.0422, time= 12.13sec, best_epoch=  266, per_error=9.0192\n",
      "epoch= 280, train_loss= 0.0427, test_loss= 0.0419, time= 12.18sec, best_epoch=  274, per_error=8.9516\n",
      "epoch= 290, train_loss= 0.0429, test_loss= 0.0437, time= 12.16sec, best_epoch=  287, per_error=9.1073\n",
      "epoch= 300, train_loss= 0.0420, test_loss= 0.0405, time= 12.39sec, best_epoch=  300, per_error=8.6958\n",
      "epoch= 310, train_loss= 0.0409, test_loss= 0.0425, time= 12.34sec, best_epoch=  300, per_error=8.8890\n",
      "epoch= 320, train_loss= 0.0420, test_loss= 0.0396, time= 12.10sec, best_epoch=  320, per_error=8.5098\n",
      "epoch= 330, train_loss= 0.0400, test_loss= 0.0406, time= 12.35sec, best_epoch=  322, per_error=8.3478\n",
      "epoch= 340, train_loss= 0.0402, test_loss= 0.0406, time= 12.26sec, best_epoch=  338, per_error=8.5443\n",
      "epoch= 350, train_loss= 0.0399, test_loss= 0.0396, time= 12.34sec, best_epoch=  347, per_error=8.3409\n",
      "epoch= 360, train_loss= 0.0391, test_loss= 0.0394, time= 12.24sec, best_epoch=  355, per_error=8.0907\n",
      "epoch= 370, train_loss= 0.0394, test_loss= 0.0388, time= 12.00sec, best_epoch=  362, per_error=8.0682\n",
      "epoch= 380, train_loss= 0.0394, test_loss= 0.0384, time= 12.16sec, best_epoch=  376, per_error=8.1778\n",
      "epoch= 390, train_loss= 0.0377, test_loss= 0.0387, time= 12.45sec, best_epoch=  385, per_error=7.8500\n",
      "best_test_loss= 0.0375, best_epoch=  398, time= 504.69sec\n"
     ]
    }
   ],
   "source": [
    "_,_ = singal_train_CNN('WEIYI_Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcaff8c-0cab-4649-8e68-4895ed4cf2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
